# AHC029

## ToDo

- ルールベース(Done)
  - 使用する方針カード選択
    - 増資(t=4)
    - 労働(t=0,1): 以下の大きい方を選択
      - t=0: w
      - t=1: w*M
    - 0番目を使用
  - プロジェクト選択: 残務量に対する価値が大きい方を選択
    - t=0
      - v/hが一番大きいプロジェクトを選択
      - ただしv/hが一定以下の場合はt=2, 3の方針カードを利用
    - t=2
      - v/hが一番小さいプロジェクトを選択
  - 新しい方針カード選択
    - 増資(t=4)
    - 労働(t=0,1)
      - 最大のw/p and w > 2*p
    - キャンセルはpが小さい物を選択
- 増資した時に、保持している方針カードを変更する。プロジェクトは自分で更新して増資を反映させる
-増資後の0の労働より大きいwのみを選択(Done)
- powershellでまとめて実行(Done)
- ルールベースのパラメータをoptunaで最適化
- 強化学習(DQNに挑戦)
  - 学習の方針
    - Agent(報酬が最大となる行動を選択)で実行した結果を貯める(行動と報酬のペアができる)
    - 学習する(報酬を一致するように学習する)
  - ToDo
    - ネットワークの構築(Done)
      - sklearnのMLPRegressorを使用(119, 32, 32, 20)
    - Agentを使っての実行(Done)
    - 学習データの蓄積(Done)
      - input: K, N, M, t, money, L, K(有無,t*5,w,p)*5, N(有無,t*5,w)*7, M(有無,h,v)*8
      - output: K*5, N*7, 8*Mのr(最終実行結果のmoneyを元に後から算出, 選択した結果の部分にrを入れる。他は0)
    - 学習の実行
    - 学習済みパラメータの読み込み
    - データを蓄積 -> 学習 を繰り返す

## history

- v0: サンプルコード(スコア: 54,598, 評価: 52,175)
- v1: 増資優先で、効率の良い労働を実施(スコア: 24,618,189, 評価: 7,686,566)
- v2: w/pが最大の評価をされていなかったバグを修正(スコア: 36,412,213, 評価: 15,467,855)
